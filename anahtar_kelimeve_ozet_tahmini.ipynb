{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_colwidth = 100 #Çıktıların boyutunu ayarlamak için."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_idf=pd.read_csv(\"dergi1.csv\",error_bad_lines=False,encoding=\"utf-8\", engine='python')\n",
    "#print(\"Veri Türleri:\\n\\n\",df_idf.dtypes)\n",
    "#print(\"Total Veri Sayısı,Sütunlar=\",df_idf.shape)\n",
    "dataframe=pd.DataFrame(df_idf)\n",
    "dataframe.columns\n",
    "\n",
    "x=dataframe[\"ozet\"]\n",
    "docs=[]\n",
    "for i in x:\n",
    "    a=str(i)\n",
    "    docs.append(a)\n",
    "docs=np.array(docs)\n",
    "classes=[]\n",
    "x=dataframe[\"anahtar_kelime\"]\n",
    "for i in x:\n",
    "    a=str(i)\n",
    "    classes.append(a)\n",
    "#docs = np.array(docs)\n",
    "df_docs = pd.DataFrame({'Dokuman': docs, \n",
    "                        'Sinif': classes})\n",
    "#df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kendi oluşturduğumuz  stopwords (689 veri)\n",
    "with open(\"stopwords.txt\",\"r+\",encoding=\"utf-8\") as dosya:\n",
    "    dizi=dosya.readlines()\n",
    "    stop_words_list=[\"\"]\n",
    "    for i in dizi:\n",
    "        i=i.replace(\"\\n\",\"\")\n",
    "        kontrol=True\n",
    "        for j in range(len(stop_words_list)):\n",
    "            if str(stop_words_list[j])==str(i):\n",
    "                kontrol=False\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        if kontrol==True:\n",
    "            stop_words_list.append(str(i))\n",
    "        else:\n",
    "            continue\n",
    "#stop_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kod bloğu hazır,bazı satırlar ekledik ve stopwordsumuzu kullandık.\n",
    "#@title Default title text\n",
    "WPT = nltk.WordPunctTokenizer()\n",
    "#stop_word_list = nltk.corpus.stopwords.words('turkish')\n",
    "stop_words_list\n",
    "def norm_doc(single_doc):\n",
    "    # TR: Dokümandan belirlenen özel karakterleri ve sayıları at\n",
    "    # EN: Remove special characters and numbers\n",
    "    single_doc = re.sub(\" \\d+\", \"\", single_doc)\n",
    "    single_doc=re.sub(\"(\\\\d|\\\\W)+\",\" \",single_doc)\n",
    "    single_doc=re.sub(\"<!--?.*?-->\",\"\",single_doc)\n",
    "    #single_doc=re.sub(\"(A-Za-z)\",\"\",single_doc)\n",
    "    pattern = r\"[{}]\".format(\",.;\") \n",
    "    single_doc = re.sub(pattern, \"\", single_doc) \n",
    "    # TR: Dokümanı küçük harflere çevir\n",
    "    # EN: Convert document to lowercase   \n",
    "    single_doc = single_doc.strip()\n",
    "    single_doc = single_doc.lower()\n",
    "    # TR: Dokümanı token'larına ayır\n",
    "    # EN: Tokenize documents\n",
    "    tokens = WPT.tokenize(single_doc)\n",
    "    # TR: Stop-word listesindeki kelimeler hariç al\n",
    "    # EN: Filter out the stop-words \n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words_list]\n",
    "    # TR: Dokümanı tekrar oluştur\n",
    "    # EN: Reconstruct the document\n",
    "    single_doc = ' '.join(filtered_tokens)\n",
    "    return single_doc\n",
    "\n",
    "norm_docs = np.vectorize(norm_doc) #like magic :)\n",
    "normalized_documents = norm_docs(docs)\n",
    "#print(normalized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABDULLAH\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ismi', 'zaman'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(min_df=0.1,max_df=0.9,stop_words=stop_words_list)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "#print(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi gram ve uni gram için gerçekleştirdim\n",
    "cv=CountVectorizer(min_df=0.,max_df=0.9,stop_words=stop_words_list,max_features=100000,ngram_range=(2,3))\n",
    "word_count_vector=cv.fit_transform(normalized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zaruret muhtaç',\n",
       " 'muhtaç vatandaşlar',\n",
       " 'vatandaşlar türkiye',\n",
       " 'türkiye kabul',\n",
       " 'kabul kişilere',\n",
       " 'yardım sosyal',\n",
       " 'sosyal adaleti',\n",
       " 'adaleti pekiştirici',\n",
       " 'pekiştirici tedbirler',\n",
       " 'tedbirler dağılımının']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#totalde anahtar kelimeler\n",
    "list(cv.vocabulary_.keys())[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #vector öğesinden yalnızca en iyi n öğelerini kullan\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # kelime indeksi ve karşılık gelen tf-idf skoru\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #özellik adının ve karşılık gelen puanın kaydını tutun\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      " Bu çalışmanın amacı üniversite öğrencilerinin öz yönetimli öğrenmeye hazırbulunuşluk düzeylerinin çeşitli değişkenler açısından incelenmesidir. Araştırmanın evrenini 20162017 eğitim öğretim yılında Bilecik Şeyh Edebali Üniversitesi’nin Fen Edebiyat Fakültesi (FEF) Mühendislik Fakültesi (MF) ve İktisadi ve İdari Bilimler Fakültesinin (İİBF) çeşitli lisans programlarında öğrenim gören 1. ve 4. sınıf öğrencileri oluşturmaktadır. Çalışmanın örneklemini ise belirtilen evrenden yansız olarak seçilen 570 öğrenci oluşturmaktadır. Araştırmada veri toplama aracı olarak kişisel bilgi formu ile Fisher ve diğerleri (2001) tarafından geliştirilen ve Türkçeye uyarlama çalışması Şahin ve Erden (2009) tarafından gerçekleştirilen “Öz Yönetimli Öğrenmeye Hazırbulunuşluk Ölçeği (ÖYÖHÖ) kullanılmıştır. Verilerin analizinde; aritmetik ortalama standart sapma gibi betimsel analiz teknikleri; iki değişkenin karşılaştırılmasında ttesti üç ve daha fazla değişkenlerin karşılaştırılmasında ise tek yönlü varyans analizi (ANOVA) testinden yararlanılmıştır. Araştırma sonucunda öğrencilerin öz yönetimli öğrenmeye hazırbulunuşluk düzeylerinin iyi düzeyde ( =3.88) olduğu belirlenmiştir. Bulgulara göre kadın öğrencilerle erkek öğrencilerin öz yönetimli öğrenmeye hazırbulunuşluk düzeylerindekadın öğrenciler lehine anlamlı fark saptanmıştır. 4. sınıf öğrencilerinin öz yönetimli öğrenmeye hazırbulunuşluk düzeyleri 1. sınıf öğrencilerine göre daha yüksektir. Öğrencilerin öz yönetimli öğrenmeye hazırbulunuşluk düzeyleri fakültelerine göre anlamlı olarak farklılaşmaktadır. Ayrıca üniversitede düzenlenen konferans panel sempozyumvb. gibi etkinliklere katılan öğrencilerin öz yönetimli öğrenmeye hazırbulunuşluk düzeyi puanlarının buetkinliklere katılmayan öğrencilerin öz yönetimli öğrenmeye hazırbulunuşluk düzeyi puanlarına göre; bu etkinliklere bazen katılan öğrencilerin puanlarının da hiç katılmayan öğrencilerin puanlarına göre daha yüksek olduğu belirlenmiştir. \n",
      "\n",
      "===Keywords===\n",
      "öğrenmeye hazırbulunuşluk 0.33\n",
      "öz yönetimli öğrenmeye 0.33\n",
      "öz yönetimli 0.33\n",
      "yönetimli öğrenmeye hazırbulunuşluk 0.33\n",
      "yönetimli öğrenmeye 0.33\n"
     ]
    }
   ],
   "source": [
    "# you only needs to do this once, this is a mapping of index to \n",
    "feature_names=cv.get_feature_names()\n",
    "docs_test=df_idf['ozet'].tolist() \n",
    "# get the document that we want to extract keywords from\n",
    "doc=docs_test[1]\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    " \n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    " \n",
    "# now print the results\n",
    "print(\"\\n=====Doc=====\")\n",
    "print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])\n",
    "\n",
    "#1 dergiyi tf_idf ile hesapladım.2 anahtar kelimeden brini buldum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
